{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dinov2Model(\n",
      "  (embeddings): Dinov2Embeddings(\n",
      "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
      "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): Dinov2Encoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-23): 24 x Dinov2Layer(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attention): Dinov2Attention(\n",
      "          (attention): Dinov2SelfAttention(\n",
      "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): Dinov2SelfOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (layer_scale1): Dinov2LayerScale()\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Dinov2MLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "        (layer_scale2): Dinov2LayerScale()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 使用transformers库中的DINOv2模型\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "\n",
    "# load model and processor\n",
    "checkpoint = \"/media/cartolab3/DataDisk/wuqilong_file/Projects/RerenkVPR/pretrained_model/dinov2_large\"\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2\n",
    "from einops import rearrange\n",
    "from dino_visual.visualization_tools import get_pca_map\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# input_image_path\n",
    "input_image_path = \"imgs/00011.jpg\"\n",
    "image_size = (322,322)\n",
    "\n",
    "def visualize_dinov2(input_image_path, image_size=(322,322)):\n",
    "    # 定义加载图像并resize，输出tensor的transform\n",
    "    ori_image = Image.open(input_image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image=transform(ori_image).unsqueeze(0)\n",
    "    print(image.shape)\n",
    "\n",
    "    # 输入模型\n",
    "    outputs = model(image)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # 获取特征图\n",
    "    features = outputs.last_hidden_state\n",
    "    features = rearrange(features[:,1:,:], 'b (h w) c -> b h w c', h= int(image_size[0]/14), w=int(image_size[1]/14))\n",
    "    features = features[0].float()\n",
    "\n",
    "    # 设置重采样图大小\n",
    "    color = get_pca_map(features.detach(), image_size, interpolation='bilinear')\n",
    "    color=color*255\n",
    "\n",
    "    # 将color代表的图像和原始图像拼接，形成一行两列的图像\n",
    "    image = np.hstack((ori_image.resize(image_size), color))\n",
    "\n",
    "    # 定义输出文件名称\n",
    "    output_file = f'{os.path.dirname(input_image_path)}/{os.path.basename(input_image_path).split(\".\")[0]}_dinov2.jpg'\n",
    "\n",
    "    cv2.imwrite(output_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n",
      "torch.Size([1, 3, 322, 322])\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 可视化多张图像\n",
    "img_path=\"./imgs/output_imgs/pitts\"\n",
    "image_list = glob.glob(os.path.join(img_path, \"*.jpg\"))\n",
    "print(len(image_list))\n",
    "for image_path in image_list:\n",
    "    visualize_dinov2(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuqilong_lighting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
